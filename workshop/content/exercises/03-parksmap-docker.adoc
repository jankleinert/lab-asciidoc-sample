In this lab, we're going to deploy the web component of the ParksMap application which is also called `parksmap` and uses OpenShift's service discovery mechanism to discover the backend services deployed and shows their data on the map.

image::roadshow-app-architecture-parksmap-1.png[Application architecture,800,align="center"]

== Exercise: Deploying your First Image

Let's start by doing the simplest thing possible - get a plain old
Docker-formatted image to run on OpenShift. This is incredibly simple to do.
With OpenShift it can be done directly from the web console.

Return to the web console:

*link:%console_url%[]*

CAUTION: In the following steps, replace `%project_namespace%` with the project provided to you.

Find your *%project_namespace%* project and click it. On the next page, click *Add* and a drop down menu will appear. There are several options, but we are only concerned with *Deploy Image*. Click *Deploy Image* in the *Add* drop down.  

image::parksmap-add-to-project-dialog.png[Add to Project]

This will open a dialog that will allow you to specify the information for the image you want to deploy. In the *Image Name* field, copy/paste the following into the box:

[source]
----
docker.io/openshiftroadshow/%PARKSMAP_IMAGENAME%:%PARKSMAP_VERSION%
----

Your screen will end up looking something like this:

image::parksmap-image.png[Explore Project]

Either press *enter* or click on the magnifying glass. OpenShift will then go
out to the container registry specified and interrogate the image. You then are
presented with some options to add environment variables -- which we will learn about later.


Make sure to have the correct application name:

image::parksmap-image-options.png[Explore Project]

Next, click the blue *Deploy* button. You will be directed to the Project Status page, 
where you should see the entry for the `parksmap` deployment config. 

These few steps are the only ones you need to run to get a
container image deployed on OpenShift. This should work with any
container image that follows best practices, such as defining an EXPOSE
port, not needing to run specifically as the *root user* or other user name, and
a single non-exiting CMD to execute on start. 

== Background: Containers and Pods

Before we start digging in, we need to understand how containers and *Pods* are
related. We will not be covering the background on these technologies in this lab but if you have questions please inform the instructor. Instead, we will dive right in and start using them.

In OpenShift, the smallest deployable unit is a *Pod*. A *Pod* is a group of one or more OCI containers deployed together and guaranteed to be on the same host.
From the official OpenShift documentation:

[quote]
__
Each *Pod* has its own IP address, therefore owning its entire port space, and
containers within pods can share storage. *Pods* can be "tagged" with one or
more labels, which are then used to select and manage groups of *pods* in a
single operation.
__

*Pods* can contain multiple OCI containers. The general idea is for a *Pod* to
contain a "main process" and any auxiliary services you want to run along with that process. Examples of containers you might put in a *Pod* are, an Apache HTTPD
server, a log analyzer, and a file service to help manage uploaded files.

== Exercise: Examining the Pod

In the web console's project status page you will see that there is a single *Pod* that
was created by your actions. 

image::parksmap-overview.png[Pod overview]

You can also get a list of all the *Pods* created within your *Project*, by navigating to 
*Workloads -> Pods*

image::parksmap-podlist.png[Pod list]

This *Pod* contains a single container, which
happens to be the `parksmap` application - a simple Spring Boot/Java application.

You can also examine *Pods* from the command line:

[source,bash,role=execute-1]
----
oc get pods
----

You should see output that looks similar to:

[source,bash]
----
NAME                READY   STATUS      RESTARTS   AGE
parksmap-1-deploy   0/1     Completed   0          4m56s
parksmap-1-gxbgq    1/1     Running     0          4m48s
----

The above output lists all of the *Pods* in the current *Project*, including the
*Pod* name, state, restarts, and uptime. Once you have a *Pod*'s name, you can
get more information about the *Pod* using the `oc get` command.  To make the
output readable, I suggest changing the output type to *YAML* using the
following syntax:

NOTE: Make sure you use the correct *Pod* name from your output.

[source,bash,role=copy-and-edit]
----
oc get pod parksmap-1-gxbgq -o yaml
----

You should see something like the following output (which has been truncated due
to space considerations of this workshop manual):

[source,bash]
----
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks-status: |-
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.130.2.68"
          ],
          "default": true,
          "dns": {}
      }]
    openshift.io/deployment-config.latest-version: "1"
    openshift.io/deployment-config.name: parksmap
    openshift.io/deployment.name: parksmap-1
    openshift.io/generated-by: OpenShiftWebConsole
    openshift.io/scc: restricted
  creationTimestamp: 2019-05-20T17:45:45Z
  generateName: parksmap-1-
  labels:
    app: parksmap
    deployment: parksmap-1
    deploymentconfig: parksmap
  name: parksmap-1-gxbgq
...............
----

The web interface also shows a lot of the same information on the *Pod* details
page. If you click on the name of the *Pod*, you will
find the details page. You can also get there by clicking *Workloads -> Pods* and then clicking the *Pod* name.

image::parksmap-pod.png[Pod list]

Getting the `parksmap` image running may take a little while to complete. Each
OpenShift node that is asked to run the image has to pull (download) it, if the
node does not already have it cached locally. You can check on the status of the
image download and deployment in the *Pod* details page, or from the command
line with the `oc get pods` command that you used before.

== Background: Customizing the Image Lifecycle Behavior

Whenever OpenShift asks the node's CRI (Container Runtime Interface) runtime (Docker daemon or CRI-O) to run an image, the runtime will check to make sure it has the right "version" of the image to run.
If it doesn't, it will pull it from the specified registry.

There are a number of ways to customize this behavior. They are documented in
https://%DOCS_URL%/dev_guide/application_lifecycle/new_app.html#specifying-an-image[specifying an image]
as well as
https://%DOCS_URL%/dev_guide/managing_images.html#image-pull-policy[image pull policy].

== Background: Services

*Services* provide a convenient abstraction layer inside OpenShift to find a
group of similar *Pods*. They also act as an internal proxy/load balancer between
those *Pods* and anything else that needs to access them from inside the
OpenShift environment. For example, if you needed more `parksmap` instances to
handle the load, you could spin up more *Pods*. OpenShift automatically maps
them as endpoints to the *Service*, and the incoming requests would not notice
anything different except that the *Service* was now doing a better job handling
the requests.

When you asked OpenShift to run the image, it automatically created a *Service*
for you. Remember that services are an internal construct. They are not
available to the "outside world", or anything that is outside the OpenShift
environment. That's okay, as you will learn later.

The way that a *Service* maps to a set of *Pods* is via a system of *Labels* and
*Selectors*. *Services* are assigned a fixed IP address and many ports and
protocols can be mapped.

There is a lot more information about
https://%DOCS_URL%/architecture/core_concepts/pods_and_services.html#services[Services],
including the YAML format to make one by hand, in the official documentation.

Now that we understand the basics of what a *Service* is, let's take a look at
the *Service* that was created for the image that we just deployed. In order to
view the *Services* defined in your *Project*, enter in the following command:

[source,bash,role=execute-1]
----
oc get services
----

You should see output similar to the following:

[source,bash]
----
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
parksmap   ClusterIP   172.30.169.213  <none>        8080/TCP   3h
----

In the above output, we can see that we have a *Service* named `parksmap` with an
IP/Port combination of 172.30.169.213/8080TCP. Your IP address may be different, as
each *Service* receives a unique IP address upon creation. *Service* IPs are
fixed and never change for the life of the *Service*.

In the web console, service information is available by clicking *Networking -> Services*.

image::parksmap-serviceslist.png[Services list]

You can also get more detailed information about a *Service* by using the
following command to display the data in YAML:

[source,bash,role=execute-1]
----
oc get service parksmap -o yaml
----

You should see output similar to the following:

[source,bash]
----
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftWebConsole
  creationTimestamp: 2019-05-20T17:45:36Z
  labels:
    app: parksmap
  name: parksmap
  namespace: user2
  resourceVersion: "5539180"
  selfLink: /api/v1/namespaces/user2/services/parksmap
  uid: 12f10c9a-7b27-11e9-853e-0a3e73a24f9c
spec:
  clusterIP: 172.30.250.179
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    deploymentconfig: parksmap
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

Take note of the `selector` stanza. Remember it.

Alternatively, you can use the web console to view information about the *Service*.

image::parksmap-service.png[Service]

It is also of interest to view the YAML of the *Pod* to understand how OpenShift
wires components together. For example, run the following command to get the
name of your `parksmap` *Pod*:

[source,bash,role=execute-1]
----
oc get pods
----

You should see output similar to the following:

[source,bash]
----
NAME                READY   STATUS      RESTARTS   AGE
parksmap-1-deploy   0/1     Completed   0          12m
parksmap-1-gxbgq    1/1     Running     0          12m
----

Now you can view the detailed data for your *Pod* with the following command:

[source,bash,role=copy-and-edit]
----
oc get pod parksmap-1-gxbgq -o yaml
----

Under the `metadata` section you should see the following:

[source,bash]
----
  labels:
    app: parksmap
    deployment: parksmap-1
    deploymentconfig: parksmap
----

* The *Service* has `selector` stanza that refers to `deploymentconfig=parksmap`.
* The *Pod* has multiple *Labels*:
** `app=parksmap`
** `deployment=parksmap-1`
** `deploymentconfig=parksmap`

*Labels* are just key/value pairs. Any *Pod* in this *Project* that has a *Label* that
matches the *Selector* will be associated with the *Service*. To see this in
action, issue the following command:

[source,bash,role=execute-1]
----
oc describe service parksmap
----

You should see something like the following output:

[source,bash]
----
Name:              parksmap
Namespace:         {{ USER_PROJECT }}
Labels:            app=parksmap
Annotations:       openshift.io/generated-by: OpenShiftWebConsole
Selector:          deploymentconfig=parksmap
Type:              ClusterIP
IP:                172.30.250.179
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.130.2.68:8080
Session Affinity:  None
Events:            <none>
----

You may be wondering why only one endpoint is listed. That is because there is
only one *Pod* currently running.  In the next lab, we will learn how to scale
an application, at which point you will be able to see multiple endpoints
associated with the *Service*.
